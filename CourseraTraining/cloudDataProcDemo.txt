Let me show you how to create
a Cloud Dataproc cluster, modify the number of
workers in the cluster, and submit a simple
Apache Spark job. So here I'm in the GCP console. The first thing I want to do is navigate to Cloud Dataproc. That's pretty far down,
so let's navigate down to Big Data,
we have Dataproc. It's going to check if
there's already a cluster which we don't have so we can go ahead and now create a cluster. We could start off by
defining the name. Let's just call it
our example-cluster. Then I'm not going
to change any of the other settings but just
kind of highlight them. We can define where it is stored, what regions and zones. What kind of mode, which defines the relationship between
nodes and workers. We want to have one
master and workers. You can also have a high
availability setting, where you have three
masters and then define the name of the workers. You have the machine types available for the master nodes, so four virtual CPUs. Then we also have the workers. There also can be
four virtual CPUs, there's going to be two of them. So in total disk
itself is going to create 12 virtual CPUs. If we go to the advanced options, we could make some of
these nodes preemptible. We can define the
network, the subnetworks, network tags in terms
of firewall rules. Make this internal IP only, a Cloud Storage bucket
for staging image. You can see there are
lots of other options all the way down to the
specific encryption. So let me go ahead
and just create this with the default configuration. Click Create, and
again this is going to create a bunch of different
machines for us now. If I open another tab and actually navigate
to Compute Engine, we will see all those instances
being generated for us. So I can go to Compute Engine. So even though this
is a managed service, we can see all of the
instances there already. So we have the master and we
have our two worker nodes. They just take the name that I specified and then
touches M for master, W for worker and starts
with a zero index. So if I come back
here, I can refresh. The clustered self is
still being initialized. The software that's being
installed and all the setup that's happening
in the back end. Once the cluster's ready, we can go ahead and we
could maybe resize that. We see that we currently have two worker nodes
and we could change it to something else like
maybe three worker nodes. Then after that we're
actually going to go ahead and submit a job for this. So here we are just took
another minute or two, we have the cluster
up and running. I can go click on the cluster itself and I can get more
information about it. So here we have all sorts
of monitoring setup. If I go to the VM
instances I'll see those. I can SSH the master, any jobs I have which currently
we don't have any yet. If I click on the configuration, we'll see that we currently
have two worker nodes. If I click on edit,
I can change that. So let's say we want
three worker nodes, can change it to
three and hit Save. It's now going to go ahead and request that update for us. So it's going to
create another worker and it's also going
to update the master, let the master to know that there's another
worker out there. So when we submit jobs, all the workers are
being leveraged. So if I change back
to Compute Engine, here we see the new workers
already up and running. If I come back here and refresh, you can see that the cluster itself is still being updated. This again should just take a
minute or two, pretty fast. Again this is a managed
service but we can see the actual back end instances
that are being leveraged. So here we can see the
cluster update is complete. I can click on it again and go to the configuration and we can see that we now have
three worker nodes. So time to submit a job. Let's go to the job section
and click on submit a job. I can leave the job
ID, leave the region. Obviously you want to
select the cluster, especially if I had
multiple clusters. The job type in this case
is going to be Spark. I'm going to define a main class. This is just from
the example class. What we're going to do actually
is we're going to provide an example to calculate
the value of Pi. So arguments, I'm just
going to give it a thousand and JAR file, I'm going to provide
that as well. Then I can review that, there's lots of other
things I have properties, labels. So I'm all set. So I'm going to click
submit on this job. It's going to go ahead
and submit that. That job is now running that's the status symbol that's
on here right now. I can go click on
that job itself. Here I can see the
job actually running. I can also review the
configuration one more time. So here see all the
different settings that I just specified. We can go back to output. Again this is now going to do
a rough calculation for us to estimate the value of Pi. So we'll just wait for that. Here we go. It says that
Pi is roughly this. So the job is now complete. If this is all that
we wanted to do, we could go ahead and
delete the cluster. Otherwise, we could
submit more jobs. In our case, we're done. So let's go back to the cluster, select that, and click Delete. It's going to lead also all
the data, can't undo this. Okay. Click that, and you can go to Compute
Engine, refresh here. We can already see that
all of these are now being stopped and will then be deleted. That way you can easily spin
up clusters and delete them so that you're only being
charged for the uses of the cluster while you need it. So we can wait around
for this to be deleted. So that just took
another minute or two. We can see that the
cluster itself was deleted and if I go
to the instances, we can also see that all
the instances are gone. That's how easy it is to create a Cloud Dataproc cluster and submit a job to that cluster.