Now let's talk about HTTPS load balancing, which acts at Layer 7 of the OSI model. This is the application layer which deals with the actual content of each message, allowing for routing decisions be based on the URL. We will also talk about Cloud Armor, and Cloud CDN in this section. GCP's HTTPS load balancing provides global load balancing for HTTPS requests, destined for your instances. This means that your applications are available to your customers, at a single anycast IP address, which simplifies your DNS setup. HTTPS load balancing balances HTTP, and HTTPS traffic, across multiple backend instances, and across multiple regions. HTTP requests are load balanced on port 80, or 8080, and HTTPS requests, are load balanced on port 443. This load balancer supports both IPv4 and IPv6 clients, is scalable, requires no pre-warming, and enables content-based and cross region load balancing. You can configure URL maps that route some URLs, to one set of instances, and route another URL to other instances. Requests are generally routed to the instance group that is closest to the user. If the closest instance group does not have sufficient capacity, the request is sent to the next closest instance group that does have capacity. You will get to explore most of these benefits, in the first lab of the module. Let me walk you through the complete architecture, of an HTTPS load balancer, by using this diagram. A global forwarding rule directs requests from the Internet, to a target HTTP proxy. The target HTTP proxy, checks each request against a URL map, to determine the appropriate backend service, for the request. For example, you can send requests for www.example.com/audio, to one backend service, which contains instances configured to deliver audio files, and requests for www.example.com/video, to another backend service which contains instances configured to deliver video files. The backend service on the other hand, direct each request to the appropriate backend based on serving capacity, zone, an instance health of the attached backends. The backends service, contain a health check, session affinity, a timeout setting, and one or more backends. A health check polls instances attached to the backend service, at configured intervals. Instances that pass the health check, are allowed to receive new requests. Unhealthy instances are not sent requests until they are healthy again. Normally, HTTPS load balancing uses a round robin algorithm to distribute requests among available instances. This can be overwritten with session affinity. Session affinity attempts to send all requests from the same client to the same virtual machine instance. Backend services also have a timeout setting, which is set to 30 seconds by default. This is the amount of time the backend service will wait on the backend, before considering the request a failure. This is a fixed time out, not an idle time out. If you require longer lift connections, set the value appropriately. The backends themselves, contain an instance group, a balancing mode, and a capacity scaler. An instance group contains virtual machine instances. The instance groups maybe a managed instance group, with or without auto-scaling, or unmanaged instance group. A balancing mode tells the load balancing system how to determine when the backend is at full usage. If all the backends for the backend service in a region are at full usage, new requests are automatically routed to the nearest region that can still handle requests. The balanced mode can be based on CPU utilization, or requests per second or RPS. A capacity setting is an additional control, that interacts with the balancing mode setting. For example, if you normally want your instances to operate at a maximum of 80 percent CPU utilization, you would set your balancing mode to 80 percent CPU utilization, and your capacity to 100 percent. If you want to cut instance utilization in half, you could leave the balancing mode at 80 percent CPU utilization, and set capacity to 50 percent. Now, any changes to your backend services are not instantaneous, so don't be surprised if it takes several minutes for your changes to propagate, through the network