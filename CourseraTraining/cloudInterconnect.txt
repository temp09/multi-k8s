Dedicated Interconnect provides direct physical connections between your On-premise
network and Google's network. This enables you to transfer a large amount of data between networks which can't be
more cost-effective than purchasing additional bandwidth
over the public Internet. In order to use
Dedicated Interconnect, you need to provision a
cross-connect between the Google network and your own router in a common
co-location facility, as shown in this diagram. To exchange routes
between the networks, you configure a BGP session over the interconnect between
the Cloud Router and the On-premise router. This will allow user traffic from the on-premise network to reach GCP resources on the
VPC network and vice-versa. Dedicated Interconnect can be
configured to offer a 99.9 percent or a 99.99
percent uptime SLA. See the Dedicated
Interconnect documentation for details on how to
achieve these SLAs. In order to use
Dedicated Interconnect, your network must physically meet Google's network in a supported
co-location facility. This map shows the location where you can create
dedicated connections. For a full list of
these locations, see the link section
of this video. Now you might look
at this map and say well I am nowhere near
one of these locations. That's when you want to consider
a Partner Interconnect. Partner Interconnect provides
connectivity between your on-premise network and your VPC network through a
supported service provider. This is useful if
your data center is in the physical location
that cannot reach a Dedicated Interconnect
co-location facility or if your data needs don't warrant a Dedicated Interconnect. In order to use
Partner Interconnect, you work with the supported
service provider to connect your VPC and
on-premise networks. For a full list of providers, see the link section
of this video. These service providers have existing physical connections to Google's network that they make available for their
customers to use. After you establish connectivity with the service provider, you can request a Partner
Interconnect connection from your service provider then establish a BGP session between your Cloud Router and
On-premise Router to start passing traffic
between your networks via the service
providers network. Partner Interconnect can
be configured to offer a 99.9 percent or 99.99 percent uptime SLA between Google and
the service provider. See the Partner Interconnect
documentation for details on how to
achieve these SLAs. Let me compare the
Interconnect options that we just discussed. All of these options provide
internal IP address access between resources in
your On-premise network and in your VPC network. The main differences are the connection capacity and the requirements for
using a service. The IPSec VPN tunnels that Cloud VPN offers
have a capacity of 1.5 to 3 Gbps per tunnel and require VPN device on
your On-premise network. The 1.5 Gbps capacity applies to the traffic
that traverses the public Internet and
the three Gbps capacity applies to the traffic that is traversing a Direct Peering link. You can configure
multiple tunnels if you want to scale
this capacity. Dedicated Interconnect has
a capacity of 10 Gbps per link and requires you to have a connection in a Google
supported co-location facility. You can have up to eight
links to achieve multiples of 10 Gbps by 10 Gbps is
the minimum capacity. As of this recording, there is a Beta
feature that provides 100 Gbps per link with
a maximum of two links. Keep in mind that
features that are in beta are not covered by any SLA or deprecation
policies and might be subject to
backward-incompatible changes. Partner Interconnect has
a capacity of 50 Mbps to 10 Gbps per connection and requirements depend on
the service provider. My recommendation is to
start with VPN tunnels. When you need enterprise-grade
connection to GCP, switch to Dedicated Interconnect or Partner Interconnect
depending on your proximity to a
co-location facility and your capacity requirements.