Hello. In this lesson, we will discuss failover and disaster recovery on the Apigee platform. This lesson uses a two-region planet for illustration. Many of these scenarios also apply to planets with one region or more than two regions. The examples given here show what is possible but this is not an exhaustive list of all failures. Your operational practices will inform how to handle these and other outage situations. Apigee Edge is designed as an active/active system. Data replicates continuously across regions in the planet. That means that you can run traffic through all regions simultaneously. This is done through a global load balancing mechanism which is usually based on DNS and may take advantage of geolocation to route requests to the closest available Apigee region. Regions will also have a local load balancer to distribute requests to routers once they arrive. With this in mind, let's discuss some outage examples to see how the system will respond and which actions you will need to take to recover. If a single management node is down, the management services on it will be unreachable. This will not impact API traffic. Since there is another management node in region two, management services will still be available if they are properly load balanced. In this situation, bring the node up and start Apigee services. If the node can't be recovered, you can restore from backups or reinstall. If you must reinstall, be sure to clean up the old management server registration from the system. If a second management server node goes down while the first one is already offline, management services for the planet will be affected. API traffic will continue to flow normally but operations such as user management, API deployment, and organization creation will be temporarily unavailable. To recover, bring one of the two nodes back online. If that is not possible, recover from backups or reinstall. In this example, a single Cassandra node is offline. This will make the Cassandra and Zookeeper processors unreachable. There is no direct impact except that maximum Cassandra performance is slightly reduced. If you can, bring the node back online. If that is not possible, the easiest way to recover is to decommission and reinstall the node. Since Cassandra can stream data from other nodes when it is installed, this is often easier than restoring from backups. In the next example, we see one Cassandra node offline in each region. The impact of this situation is identical to the previous one. Other than a slight reduction in maximum Cassandra performance, there is no impact. As before, bring the node back online if possible. If that is not possible, decommission and reinstall the nodes or recover from backups. The situation shown here is different from the previous two. While there are still only two Cassander nodes down, in this case, there will be an impact because they're in the same region. Since a majority of local Cassandra nodes must be online in each region for Cassandra to be available, region one is effectively down. You will need to fillover traffic from region one to region two during the Cassandra outage to continue servicing API requests. Recovery is similar to the previous two examples, bring the nodes up and start services. If that is not possible, decommission and reinstall, or recover from backups. Here, a single router and message processor node is down. This will have no functional impact on the region but it will reduce available gateway capacity somewhat. Your load balancing infrastructure should automatically remove the node from the load balancing pool. To recover, bring the node back online. If the node is lost, restore from backups or install a new node. Note that if you have to install a new node, you will need to clean up existing router and message processor registrations in the management API. This will usually mean removing message processors from any environment with which they're associated and then deleting the UUIDs for the router and message processor entirely. If all routers and message processors in a region are down, that region will be unable to respond to API requests. You should failover traffic to region two until you have routers and message processors available in region one again. To recover, bring the nodes back online. Be careful not to route traffic back to region one until you ensure that you have enough processing capacity online to handle the traffic. If you can't bring the nodes back online, recover from backups or reinstall the nodes. In the next example, we see a single Qpid node offline. There will be no impact to API traffic or management services. Assuming there is sufficient capacity and other Qpid nodes to handle all analytics messages, there will also be no impact to analytic services. The usual methods could be used to recover. If bringing the node back online is not possible, recover from backups or reinstall. If you have to reinstall, you will need to remove the old Qpid server UUID from its analytics group and then delete the old Qpid server registration. In this situation, both Qpid nodes in one region are down. If no queues are available to receive messages, the message processors will store a small amount of data in memory buffers and then drop the messages when the buffers run out. With all Qpid nodes down in a region, you are likely to lose some analytics data for that region, since API traffic is not impacted, a failover to region two is optional but may be a good idea if you expect an extended outage and need to see analytics data. Recovery is identical to a single offline Qpid node. Bring the nodes back online, recover from backups, or reinstall. If you lose a PostgreSQL you will node, the impact depends on whether that node is the master or standby database. In both cases, there will be no impact on API traffic or management services. If you lose a master database, the immediate impact will be out of date analytics data. Since the writeable database is off line, analytics data will buffer in the Qpid queues awaiting ingestion. As long as those queues have capacity remaining, no analytics data will be lost. To allow data ingestion to continue, promote the standby database to master. This will change the database from read-only to read-write, and all Qpid queues will be flushed to the master database. Be aware that once you have promoted a standby to master, you will need to reconfigure the old master as a standby database and replicate new data back to it once it comes online. In environments with extremely large analytics datasets, it may be preferable to simply troubleshoot the master database server rather than promoting a standby and replicating data back to the older master. The threshold for this depends on how quickly you believe the master can be recovered and how critical your analytics reports are. If you are unable to recover the master, you generally do not need to recover from backups as long as a standby database is available with your data set. Promote the standby, reinstall the old master as a standby, then replicate the data from the new master database to populate it. This situation is even simpler if the standby database is down. No failover is necessary and recovery will consist of bringing the standby up or rebuilding and replicating data. There will also be no impact to API traffic, management services, or analytic services as a standby database operates as a read-only replica and it's not necessary for any other services to function. The loss of an entire region is rare but it occasionally occurs due to a power loss, network link cut, or other failure. If this happens, failover traffic to the other region. As part of your capacity planning, you should ensure that you can sustain the loss of any one region without overwhelming the other regions. Recovery will depend on whether the region became temporarily unavailable or all nodes in the region were lost. If the region is temporarily unavailable, no intervention is required. Once the region becomes available again, services will synchronize data and you can send API traffic back to the region. If all nodes were lost, you will need to follow the recovery procedures detailed throughout this lesson or restore the entire region from backups. If a power loss caused all nodes to power cycle, the outage may occur simply because services did not start when the nodes booted. If this happens, you can recover by starting services in the correct order. First, start all open source components. This will include Cassandra, Zookeeper, PostgreSQL, QpidD and Openldap. Next, start the management server, message processor, PostgreSQL server, Cupid server, and router. Finally, start the Edge UI. It is extremely unlikely that you will lose all regions simultaneously. The most common cause is human error. A faulty infrastructure change or network misconfiguration could affect many regions simultaneously. To protect against this, follow change configuration best practices and stagger rollouts to reduce the scope of impact from large scale infrastructure changes. Recovery will be identical to the loss of a single region, depending on the reason for the outage, start services or recover any loss nodes from backups. Follow the same start order detailed in the previous example but start each service across regions. For instance, start Cassandra in all regions before proceeding onto the next service. For more information on this topic, refer to our documentation. If you have any questions, please post them on our community. Thanks for watching.