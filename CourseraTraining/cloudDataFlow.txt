Let's learn a little bit
about Cloud Dataflow. Cloud Dataflow is a
managed service for executing a wide variety of
data processing patterns. It is essentially a
fully managed service for transforming
and enriching data in stream and batch modes with equal reliability
and expressiveness. With Cloud Dataflow, a
lot of the complexity of infrastructure setup and
maintenance is handled for you. It's build on Google
Cloud Infrastructure and auto-scaled to meet the
demands of your data pipeline, allowing you to intelligently scale to millions of
queries per second. Cloud Dataflow supports fast, simplified pipeline development
via expressive SQL, Java, and Python APIs in the Apache Beam SDK which provides a rich
set of windowing, and session analysis
primitives as well as an ecosystem of source
and sync connectors. Cloud Dataflow, is also tightly coupled with other GCP
services like Stackdriver, so you can set a priority
alerts and notifications to monitor your pipeline and the quality of data
coming in and out. This diagram shows some example use cases of Cloud Dataflow. As I just mentioned, Cloud Dataflow processes
stream and batch data. This data could come from
other GCP services like Cloud Datastore or Cloud Pub Sub which is Google's messaging
and publishing service. The data could also
be ingested from third party services like
Apache Avro and Apache Kafka. After you transform the
data with Cloud Dataflow, you can analyze it in BigQuery, AI platform, or even
Cloud Bigtable. Using Data Studio,
you can even build real-time dashboards
for IoT devices